{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Splice_logo.jpeg\" width=\"250\" height=\"200\" align=\"left\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train machine learning models using the Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you find features values at the correct point in time?\n",
    "   - ### Features are updated at different times\n",
    "   - ### How would you join across asynchronous timestamps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/point_in_time_problem.png\" width=\"900\" align=\"left\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can be done without a Feature Store once or twice, but for 5 or 50 models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily build point in time consistent training sets with our Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/training_set.png\" width=\"1000\"  align=\"left\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Feature Set Tables\n",
    "<img src=\"Images/FS_tables.png\" width=\"800\" height=\"400\" align=\"left\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Create pysplice context. Allows you to create a Spark dataframe using our Native Spark DataSource \n",
    "from splicemachine.spark import PySpliceContext\n",
    "splice = PySpliceContext(spark)\n",
    "\n",
    "#Initialize our Feature Store API\n",
    "from splicemachine.features import FeatureStore\n",
    "from splicemachine.features.constants import FeatureType\n",
    "fs = FeatureStore(splice)\n",
    "\n",
    "#Initialize MLFlow\n",
    "from splicemachine.mlflow_support import *\n",
    "mlflow.register_feature_store(fs)\n",
    "mlflow.register_splice_context(splice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write any SQL to get your label. The label doesn't have to be apart of the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT ltv.CUSTOMERID, \n",
    "       ((w.WEEK_END_DATE - ltv.CUSTOMER_START_DATE)/ 7) CUSTOMERWEEK,\n",
    "       CAST(w.WEEK_END_DATE as TIMESTAMP) CUSTOMER_TS,  \n",
    "       ltv.CUSTOMER_LIFETIME_VALUE as CUSTOMER_LTV\n",
    "FROM retail_rfm.weeks w --splice-properties useSpark=True\n",
    "INNER JOIN \n",
    "    twimlcon_fs.customer_lifetime ltv \n",
    "    ON w.WEEK_END_DATE >= ltv.CUSTOMER_START_DATE AND w.WEEK_END_DATE <= ltv.CUSTOMER_START_DATE + 28 --only first 4 weeks\n",
    "ORDER BY 1,2\n",
    "\n",
    "{limit 8}\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training View\n",
    "#### By specifying the join key and time stamp, you can automatically get all of the relevant features you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ltv.CUSTOMERID, \n",
    "       ((w.WEEK_END_DATE - ltv.CUSTOMER_START_DATE)/ 7) CUSTOMERWEEK,\n",
    "       CAST(w.WEEK_END_DATE as TIMESTAMP) CUSTOMER_TS,  \n",
    "       ltv.CUSTOMER_LIFETIME_VALUE as CUSTOMER_LTV\n",
    "FROM retail_rfm.weeks w --splice-properties useSpark=True\n",
    "INNER JOIN \n",
    "    twimlcon_fs.customer_lifetime ltv \n",
    "    ON w.WEEK_END_DATE > ltv.CUSTOMER_START_DATE AND w.WEEK_END_DATE <= ltv.CUSTOMER_START_DATE + 28 --only first 4 weeks\n",
    "\"\"\"\n",
    "\n",
    "pks = ['CUSTOMERID','CUSTOMERWEEK'] # Each unique training row is identified by the customer and their week of spending activity\n",
    "join_keys = ['CUSTOMERID'] # This is the primary key of the Feature Sets that we want to join to\n",
    "\n",
    "fs.create_training_view(\n",
    "    'twimlcon_customer_lifetime_value',\n",
    "    sql=sql, \n",
    "    primary_keys=pks, \n",
    "    join_keys=join_keys,\n",
    "    ts_col = 'CUSTOMER_TS', # How we join each unique row with our eventual Features\n",
    "    label_col='CUSTOMER_LTV', # The thing we want to predict\n",
    "    desc = 'The current (as of queried) lifetime value of each customer per week of being a customer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easily extract all features\n",
    "#### Every time this code is re-run you have access to the most up-to-date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Dataframe\n",
    "all_features = fs.get_training_set_from_view('twimlcon_customer_lifetime_value')\n",
    "all_features.limit(8).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL used to generate the Dataframe\n",
    "sql = fs.get_training_set_from_view('twimlcon_customer_lifetime_value',return_sql=True)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Feature Selection\n",
    "As simple as using the get_training_view function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# get training set as a SQL statement\n",
    "feats = fs.get_training_view_features('twimlcon_customer_lifetime_value')\n",
    "# Grab only up to 4 weeks of RFM values\n",
    "desired_features = ['CUSTOMER_LIFETIME_DAYS'] + [f.name for f in feats if re.search('_[0-4]W',f.name)]\n",
    "\n",
    "\n",
    "\n",
    "all_features = fs.get_training_set_from_view('twimlcon_customer_lifetime_value', features = desired_features).dropna() \n",
    "\n",
    "\n",
    "top_features, feature_importances = fs.run_feature_elimination(\n",
    "    all_features,\n",
    "    features=desired_features,\n",
    "    label = 'CUSTOMER_LTV',\n",
    "    n = 10,\n",
    "    verbose=2,\n",
    "    step=30,\n",
    "    model_type='regression',\n",
    "    log_mlflow=True,\n",
    "    mlflow_run_name='Feature_Elimination_LTV',\n",
    "    return_importances=True\n",
    ")\n",
    "\n",
    "model_training_df = fs.get_training_set_from_view('twimlcon_customer_lifetime_value', features = top_features).dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Machine Learning Model\n",
    "### Splice Machine's model training is built around an integrated and enhanced version of MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.notebook import get_mlflow_ui\n",
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# SparkML Model\n",
    "###############\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler,StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "mlflow.set_experiment('Predict Lifetime Value from Initial Customer Activity')\n",
    "run_tags={'project': 'TWIMLcon Demo',\n",
    "          'team': 'INSERT YOUR NAME HERE'\n",
    "         }\n",
    "\n",
    "features_list = [f.name for f in top_features]\n",
    "features_str  = ','.join(features_list)  \n",
    "\n",
    "va = VectorAssembler(inputCols=features_list, outputCol='features_raw')\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name = f\"Regression LTV\", tags = run_tags):\n",
    "\n",
    "\n",
    "    lr = LinearRegression(featuresCol = 'features', labelCol = 'CUSTOMER_LTV', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    #lr = RandomForestRegressor(featuresCol = 'features', labelCol = 'CUSTOMER_LTV')\n",
    "    \n",
    "    pipeline = Pipeline( stages=[va, scaler, lr])\n",
    "\n",
    "    # log everything\n",
    "    mlflow.log_feature_transformations(pipeline)\n",
    "    mlflow.log_pipeline_stages(pipeline)\n",
    "\n",
    "    #train\n",
    "    train,test = model_training_df.randomSplit([0.80,0.20])\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "\n",
    "    lr_model = model.stages[-1]\n",
    "    print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "    print(\"Intercept: \" + str(lr_model.intercept))\n",
    "    \n",
    "    # log metric\n",
    "    pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"CUSTOMER_LTV\",metricName=\"r2\")\n",
    "    r2 = pred_evaluator.evaluate(predictions)\n",
    "    print(\"R Squared (R2) on test data = %g\" % r2)\n",
    "    mlflow.log_metric('r2',r2)\n",
    "\n",
    "    mlflow.log_model(model)\n",
    "    run_id = mlflow.current_run_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.notebook import get_mlflow_ui\n",
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store most important features for use in the next jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store features_list\n",
    "%store features_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
