{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steps to complete\\n1. Add the bucketization logic to the notebook\\n2. Add the cross-variable logic to the notebook\\n3. Add JSON input parameter and decode into dict\\n4. Combine cells into .py script\\n5. ingest train and test files into Splice tables\\n6. add new column for the label to the tables\\n7. add the saver-def code to save off the model\\n7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\\ninput to the python script, call the script from the stored procedure, train the model, save it\"\\n8. write a new stored procedure that deploys the model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Steps to complete\n",
    "1. Add the bucketization logic to the notebook\n",
    "2. Add the cross-variable logic to the notebook\n",
    "3. Add JSON input parameter and decode into dict\n",
    "4. Combine cells into .py script\n",
    "5. ingest train and test files into Splice tables\n",
    "6. add new column for the label to the tables\n",
    "7. add the saver-def code to save off the model\n",
    "7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\n",
    "input to the python script, call the script from the stored procedure, train the model, save it\"\n",
    "8. write a new stored procedure that deploys the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will error if you run more than once\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"model_dir\", \"\", \"Base directory for output models.\")\n",
    "flags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n",
    "                    \"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\")\n",
    "flags.DEFINE_integer(\"train_steps\", 200, \"Number of training steps.\")\n",
    "flags.DEFINE_string(\n",
    "    \"train_data\",\n",
    "    \"\",\n",
    "    \"Path to the training data.\")\n",
    "flags.DEFINE_string(\n",
    "    \"test_data\",\n",
    "    \"\",\n",
    "    \"Path to the test data.\")\n",
    "flags.DEFINE_string(\n",
    "    \"inputs\",\n",
    "    '{\"columns\": [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"gender\",\"capital_gain\",\"capital_loss\",\"hours_per_week\",\"native_country\",\"income_bracket\"],\"categorical_columns\": [\"workclass\",\"education\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"gender\",\"native_country\"],\"continuous_columns\": [\"age\",\"education_num\",\"capital_gain\",\"capital_loss\",\"hours_per_week\"],\"label_column\": \"label\",\"bucketized_columns\": {\"age_buckets\": {      \"age\": [    18,    25,    30,    35,    40,    45,    50,    55,    60,    65  ]}},\"crossed_columns\": [[  \"education\",  \"occupation\"],[  \"age_buckets\",  \"education\",  \"occupation\"],[  \"native_country\",  \"occupation\"]],\"train_data_path\": \"/Users/erindriggers/anaconda/envs/tensorflow/projects/wide_n_deep/data/train/part-r-00000.csv\",\"test_data_path\": \"/Users/admin//envs/tensorflow/projects/wide_n_deep/data/test/part-r-00000.csv\"}',\n",
    "    \"Input data dictionary\")\n",
    "flags.DEFINE_string(\"input_record\", \"\",\"Comma delimited input record\")\n",
    "flags.DEFINE_string(\"predict\", \"false\",\"Indicates if we are predicting or building the model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DICT={'continuous_columns': ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'], 'label_column': 'label', 'train_data_path': '/Users/erindriggers/anaconda/envs/tensorflow/projects/wide_n_deep/data/train/part-r-00000.csv', 'test_data_path': '/Users/admin//envs/tensorflow/projects/wide_n_deep/data/test/part-r-00000.csv', 'crossed_columns': [['education', 'occupation'], ['age_buckets', 'education', 'occupation'], ['native_country', 'occupation']], 'bucketized_columns': {'age_buckets': {'age': [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]}}, 'categorical_columns': ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country'], 'columns': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']}\n",
      "COLUMNS=['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "CATEGORICAL_COLUMNS=['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country']\n",
      "CONTINUOUS_COLUMNS=['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
      "LABEL_COLUMN=label\n",
      "COLUMNS=['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "BUCKETIZED_COLUMNS={'age_buckets': {'age': [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]}}\n",
      "CROSSED_COLUMNS=[['education', 'occupation'], ['age_buckets', 'education', 'occupation'], ['native_country', 'occupation']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TBD: The dict below should be input to teh script and constructed by a stored procedure\n",
    "## The Stored Procedure can construct JSON and then this script can decode the JSON into the dict\n",
    "## The paths to the files below should be paths constructed by a Splice Machine Export\n",
    "\n",
    "\n",
    "INPUT_DICT=json.loads(FLAGS.inputs)\n",
    "COLUMNS = INPUT_DICT['columns'];\n",
    "LABEL_COLUMN = INPUT_DICT['label_column'];\n",
    "CATEGORICAL_COLUMNS = INPUT_DICT['categorical_columns'];\n",
    "CONTINUOUS_COLUMNS = INPUT_DICT['continuous_columns'];\n",
    "CROSSED_COLUMNS = INPUT_DICT['crossed_columns'];\n",
    "BUCKETIZED_COLUMNS = INPUT_DICT['bucketized_columns'];\n",
    "\n",
    "print(\"INPUT_DICT=%s\" % INPUT_DICT)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"CATEGORICAL_COLUMNS=%s\" % CATEGORICAL_COLUMNS)\n",
    "print(\"CONTINUOUS_COLUMNS=%s\" % CONTINUOUS_COLUMNS)\n",
    "print(\"LABEL_COLUMN=%s\" % LABEL_COLUMN)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"BUCKETIZED_COLUMNS=%s\" % BUCKETIZED_COLUMNS)\n",
    "print(\"CROSSED_COLUMNS=%s\" % CROSSED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def maybe_download():\n",
    "  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n",
    "  if FLAGS.train_data:\n",
    "    train_file_name = FLAGS.train_data\n",
    "  else:\n",
    "    train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(INPUT_DICT['train_data_path'], train_file.name)  # pylint: disable=line-too-long\n",
    "    train_file_name = train_file.name\n",
    "    train_file.close()\n",
    "    print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "  if FLAGS.test_data:\n",
    "    test_file_name = FLAGS.test_data\n",
    "  else:\n",
    "    test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(INPUT_DICT['test_data_path'], test_file.name)  # pylint: disable=line-too-long\n",
    "    test_file_name = test_file.name\n",
    "    test_file.close()\n",
    "    print(\"Test data is downloaded to %s\" % test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sparse_columns(cols):\n",
    "    \"\"\"Creates tf sparse columns with hash buckets\"\"\"\n",
    "    # Sparse base columns.\n",
    "    # TBD: allow keyed columns and hash bucket size as input\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "          col, hash_bucket_size=1000)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'race': _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'marital_status': _SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'native_country': _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'relationship': _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'occupation': _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'gender': _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'education': _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), 'workclass': _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)}\n"
     ]
    }
   ],
   "source": [
    "SPARSE_TF_COLUMNS = prepare_sparse_columns(CATEGORICAL_COLUMNS)\n",
    "print(SPARSE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_continuous_columns(cols):\n",
    "    \"\"\"Creates tf.contrib.layers.real_valued_columns\"\"\"\n",
    "    #Continuous base columns\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = (tf.contrib.layers.real_valued_column(col))\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hours_per_week': _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32), 'age': _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), 'education_num': _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32), 'capital_gain': _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32), 'capital_loss': _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32)}\n"
     ]
    }
   ],
   "source": [
    "REAL_TF_COLUMNS = prepare_continuous_columns(CONTINUOUS_COLUMNS)\n",
    "print(REAL_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_buckets(cols):\n",
    "    \"\"\"Creates tf bucketed columns\"\"\"\n",
    "    new_cols = {}\n",
    "    for newCol in cols:\n",
    "        keyvalues = cols[newCol]\n",
    "        for colname in keyvalues:\n",
    "            orig_col = REAL_TF_COLUMNS[colname]\n",
    "            bound = keyvalues[colname]\n",
    "            new_cols[newCol] = tf.contrib.layers.bucketized_column(orig_col, boundaries=bound)\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age_buckets': {'age': [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]}}\n"
     ]
    }
   ],
   "source": [
    "print(BUCKETIZED_COLUMNS)\n",
    "BUCKETIZED_TF_COLUMNS = prepare_buckets(BUCKETIZED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_embedded_columns(cols):\n",
    "    \"\"\"Create tf.contrib.layers.embedding_columns for the sparse entries\"\"\"\n",
    "    tf_cols = {}\n",
    "    for col in cols:\n",
    "        tf_cols[col] = tf.contrib.layers.embedding_column(col, dimension=8)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'marital_status', 'native_country', 'relationship', 'occupation', 'gender', 'education', 'workclass']\n"
     ]
    }
   ],
   "source": [
    "print(list(SPARSE_TF_COLUMNS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDED_TF_COLUMNS = prepare_embedded_columns(list(SPARSE_TF_COLUMNS.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a6a8>), _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a7b8>), _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005aa60>), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a8c8>), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a9d8>), _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a840>), _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a950>), _SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string): _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a158>)}\n"
     ]
    }
   ],
   "source": [
    "print(EMBEDDED_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a6a8>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a7b8>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005aa60>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a8c8>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a9d8>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a840>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a950>), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x12005a158>), _RealValuedColumn(column_name='hours_per_week', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='education_num', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='capital_gain', dimension=1, default_value=None, dtype=tf.float32), _RealValuedColumn(column_name='capital_loss', dimension=1, default_value=None, dtype=tf.float32)]\n"
     ]
    }
   ],
   "source": [
    "DEEP_TF_COLUMNS =  list(EMBEDDED_TF_COLUMNS.values()) + list(REAL_TF_COLUMNS.values())\n",
    "print(DEEP_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_crossed(cols):\n",
    "    \"\"\"Creates tf crossed columns\"\"\"\n",
    "    new_cols = [];\n",
    "    for tuple in cols:\n",
    "        list_of_cols = []\n",
    "        for var in tuple:\n",
    "            b = BUCKETIZED_TF_COLUMNS.get(var,False)\n",
    "            s = SPARSE_TF_COLUMNS.get(var,False)\n",
    "            r = REAL_TF_COLUMNS.get(var,False)\n",
    "            if b : tf_var = b\n",
    "            else :\n",
    "                if s : tf_var = s\n",
    "                else :\n",
    "                    if r : tf_var = r\n",
    "            print(tf_var)\n",
    "            list_of_cols.append(tf_var)\n",
    "        new_cols.append(tf.contrib.layers.crossed_column(list_of_cols,\n",
    "                      hash_bucket_size=int(1e6)))\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n",
      "_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n",
      "_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))\n",
      "_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n",
      "_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n",
      "_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n",
      "_SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n"
     ]
    }
   ],
   "source": [
    "CROSSED_TF_COLS = prepare_crossed(CROSSED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_CrossedColumn(columns=(_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum'), _CrossedColumn(columns=(_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum'), _CrossedColumn(columns=(_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_CrossedColumn(columns=(_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(CROSSED_TF_COLS)\n",
    "CROSSED_TF_COLS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_SparseColumnHashed(column_name='race', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='marital_status', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='relationship', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='gender', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='workclass', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _CrossedColumn(columns=(_SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum'), _CrossedColumn(columns=(_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _SparseColumnHashed(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum'), _CrossedColumn(columns=(_SparseColumnHashed(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, combiner='sum')]\n"
     ]
    }
   ],
   "source": [
    "WIDE_TF_COLUMNS = list(SPARSE_TF_COLUMNS.values()) + list(BUCKETIZED_TF_COLUMNS.values()) + list(CROSSED_TF_COLS)\n",
    "print(WIDE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=WIDE_TF_COLUMNS,\n",
    "    dnn_feature_columns=DEEP_TF_COLUMNS,\n",
    "    dnn_hidden_units=[100, 50])\n",
    "  return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "#  train_file_name, test_file_name = maybe_download()\n",
    "  train_file_name=INPUT_DICT['train_data_path'];\n",
    "  test_file_name=INPUT_DICT['test_data_path'];\n",
    "\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "    \n",
    "# Temp hack - label should come in the input \n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "\n",
    "  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_outcome():\n",
    "  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "  print('model_dir = %s' % model_dir);\n",
    "  m = build_estimator(model_dir)\n",
    "\n",
    "  indata=StringIO(FLAGS.input_record)\n",
    "\n",
    "  prediction_set = pd.read_csv(\n",
    "      indata,\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=0,\n",
    "      engine=\"python\")\n",
    "\n",
    "  prediction_set[LABEL_COLUMN] = (\n",
    "      prediction_set[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "\n",
    "  y=m.predict(input_fn=lambda: input_fn(prediction_set))\n",
    "  print('Predictions: {}'.format(str(y)))\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  if FLAGS.predict == \"true\":\n",
    "    predict_outcome()\n",
    "  else:\n",
    "    train_and_eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/admin//envs/tensorflow/projects/wide_n_deep/data/test/part-r-00000.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-32cd4e19c77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/erindriggers/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-91ae7027d2d6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredict_outcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-20f478b459f5>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m       engine=\"python\")\n\u001b[1;32m     12\u001b[0m   df_test = pd.read_csv(\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOLUMNS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erindriggers/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/gfile.py\u001b[0m in \u001b[0;36mOpen\u001b[0;34m(name, mode)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mthreadsafe\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/erindriggers/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/gfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Pythonlocker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erindriggers/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/gfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, locker)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_locker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/admin//envs/tensorflow/projects/wide_n_deep/data/test/part-r-00000.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}