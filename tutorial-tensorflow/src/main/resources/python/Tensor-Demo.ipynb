{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Steps to complete\n",
    "1. Add the bucketization logic to the notebook\n",
    "2. Add the cross-variable logic to the notebook\n",
    "3. Add JSON input parameter and decode into dict\n",
    "4. Combine cells into .py script\n",
    "5. ingest train and test files into Splice tables\n",
    "6. add new column for the label to the tables\n",
    "7. add the saver-def code to save off the model\n",
    "7. write a stored procedure that exports the tables to two files, creates a JSON map that will be,\n",
    "input to the python script, call the script from the stored procedure, train the model, save it\"\n",
    "8. write a new stored procedure that deploys the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"model_dir\", \"\", \"Base directory for output models.\")\n",
    "flags.DEFINE_string(\"model_type\", \"wide_n_deep\",\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\")\n",
    "flags.DEFINE_integer(\"train_steps\", 200, \"Number of training steps.\")\n",
    "flags.DEFINE_string(\"train_data\",\"\",\"Path to the training data.\")\n",
    "flags.DEFINE_string(\"test_data\",\"\",\"Path to the test data.\")\n",
    "flags.DEFINE_string(\"inputs\",\"\",\"Input data dictionary\")\n",
    "flags.DEFINE_string(\"input_record\", \"\",\"Comma delimited input record\")\n",
    "flags.DEFINE_string(\"predict\", \"false\",\"Indicates if we are predicting or building the model\")\n",
    "flags.DEFINE_integer(\"hash_bucket_size\", 1000,\"The hash bucket size\")\n",
    "flags.DEFINE_integer(\"dimension\", 8,\"The dimension\")\n",
    "flags.DEFINE_string(\"dnn_hidden_units\", \"100,50\",\"List of hidden units per DNN layer\")\n",
    "flags.DEFINE_string(\"comparison_column\", \"income_bracket\",\"The column the value will be compared against\")\n",
    "flags.DEFINE_string(\"criteria\", \">50K\",\"The binary classification criteria\")\n",
    "\n",
    "print(\"comparison_column=%s\" % FLAGS.comparison_column)\n",
    "print(\"criteria=%s\" % FLAGS.criteria)\n",
    "print(\"hash_bucket_size=%s\" % FLAGS.hash_bucket_size)\n",
    "print(\"dimension=%s\" % FLAGS.dimension)\n",
    "print(\"dnn_hidden_units=%s\" % FLAGS.dnn_hidden_units)\n",
    "print(\"model_dir=%s\" % FLAGS.model_dir)\n",
    "print(\"model_type=%s\" % FLAGS.model_type)\n",
    "print(\"train_steps=%s\" % FLAGS.train_steps)\n",
    "print(\"train_data=%s\" % FLAGS.train_data)\n",
    "print(\"test_data=%s\" % FLAGS.test_data)\n",
    "print(\"inputs=%s\" % FLAGS.inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## TBD: The dict below should be input to teh script and constructed by a stored procedure\n",
    "## The Stored Procedure can construct JSON and then this script can decode the JSON into the dict\n",
    "## The paths to the files below should be paths constructed by a Splice Machine Export\n",
    "\n",
    "\n",
    "INPUT_DICT=json.loads(FLAGS.inputs)\n",
    "COLUMNS = INPUT_DICT['columns'];\n",
    "LABEL_COLUMN = INPUT_DICT['label_column'];\n",
    "CATEGORICAL_COLUMNS = INPUT_DICT['categorical_columns'];\n",
    "CONTINUOUS_COLUMNS = INPUT_DICT['continuous_columns'];\n",
    "CROSSED_COLUMNS = INPUT_DICT['crossed_columns'];\n",
    "BUCKETIZED_COLUMNS = INPUT_DICT['bucketized_columns'];\n",
    "DNN_HIDDEN_UNITS=[int(s) for s in FLAGS.dnn_hidden_units.split(',')] \n",
    "\n",
    "print(\"INPUT_DICT=%s\" % INPUT_DICT)\n",
    "print(\"COLUMNS=%s\" % COLUMNS)\n",
    "print(\"LABEL_COLUMN=%s\" % LABEL_COLUMN)\n",
    "print(\"CATEGORICAL_COLUMNS=%s\" % CATEGORICAL_COLUMNS)\n",
    "print(\"CONTINUOUS_COLUMNS=%s\" % CONTINUOUS_COLUMNS)\n",
    "print(\"CROSSED_COLUMNS=%s\" % CROSSED_COLUMNS)\n",
    "print(\"BUCKETIZED_COLUMNS=%s\" % BUCKETIZED_COLUMNS)\n",
    "print(\"DNN_HIDDEN_UNITS=%s\" % DNN_HIDDEN_UNITS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def maybe_download():\n",
    "  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n",
    "  if FLAGS.train_data:\n",
    "    train_file_name = FLAGS.train_data\n",
    "  else:\n",
    "    train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(FLAGS.train_data, train_file.name)  # pylint: disable=line-too-long\n",
    "    train_file_name = train_file.name\n",
    "    train_file.close()\n",
    "    print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "  if FLAGS.test_data:\n",
    "    test_file_name = FLAGS.test_data\n",
    "  else:\n",
    "    test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(FLAGS.test_data, test_file.name)  # pylint: disable=line-too-long\n",
    "    test_file_name = test_file.name\n",
    "    test_file.close()\n",
    "    print(\"Test data is downloaded to %s\" % test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sparse_columns(cols):\n",
    "    \"\"\"Creates tf sparse columns with hash buckets\"\"\"\n",
    "    # Sparse base columns.\n",
    "    # TBD: allow keyed columns and hash bucket size as input\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "          col, hash_bucket_size=FLAGS.hash_bucket_size)\n",
    "    return tf_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPARSE_TF_COLUMNS = prepare_sparse_columns(CATEGORICAL_COLUMNS)\n",
    "print(SPARSE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_continuous_columns(cols):\n",
    "    \"\"\"Creates tf.contrib.layers.real_valued_columns\"\"\"\n",
    "    #Continuous base columns\n",
    "    tf_cols ={}\n",
    "    for col in cols :\n",
    "        tf_cols[col] = (tf.contrib.layers.real_valued_column(col))\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REAL_TF_COLUMNS = prepare_continuous_columns(CONTINUOUS_COLUMNS)\n",
    "print(REAL_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_buckets(cols):\n",
    "    \"\"\"Creates tf bucketed columns\"\"\"\n",
    "    new_cols = {}\n",
    "    for newCol in cols:\n",
    "        keyvalues = cols[newCol]\n",
    "        for colname in keyvalues:\n",
    "            orig_col = REAL_TF_COLUMNS[colname]\n",
    "            bound = keyvalues[colname]\n",
    "            new_cols[newCol] = tf.contrib.layers.bucketized_column(orig_col, boundaries=bound)\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(BUCKETIZED_COLUMNS)\n",
    "BUCKETIZED_TF_COLUMNS = prepare_buckets(BUCKETIZED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_embedded_columns(cols):\n",
    "    \"\"\"Create tf.contrib.layers.embedding_columns for the sparse entries\"\"\"\n",
    "    tf_cols = {}\n",
    "    for col in cols:\n",
    "        tf_cols[col] = tf.contrib.layers.embedding_column(col, dimension=FLAGS.dimension)\n",
    "    return tf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(SPARSE_TF_COLUMNS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDED_TF_COLUMNS = prepare_embedded_columns(list(SPARSE_TF_COLUMNS.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(EMBEDDED_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEEP_TF_COLUMNS =  list(EMBEDDED_TF_COLUMNS.values()) + list(REAL_TF_COLUMNS.values())\n",
    "print(DEEP_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_crossed(cols):\n",
    "    \"\"\"Creates tf crossed columns\"\"\"\n",
    "    new_cols = [];\n",
    "    for tuple in cols:\n",
    "        list_of_cols = []\n",
    "        for var in tuple:\n",
    "            b = BUCKETIZED_TF_COLUMNS.get(var,False)\n",
    "            s = SPARSE_TF_COLUMNS.get(var,False)\n",
    "            r = REAL_TF_COLUMNS.get(var,False)\n",
    "            if b : tf_var = b\n",
    "            else :\n",
    "                if s : tf_var = s\n",
    "                else :\n",
    "                    if r : tf_var = r\n",
    "            print(tf_var)\n",
    "            list_of_cols.append(tf_var)\n",
    "        new_cols.append(tf.contrib.layers.crossed_column(list_of_cols,\n",
    "                      hash_bucket_size=int(1e6)))\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CROSSED_TF_COLS = prepare_crossed(CROSSED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(CROSSED_TF_COLS)\n",
    "CROSSED_TF_COLS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIDE_TF_COLUMNS = list(SPARSE_TF_COLUMNS.values()) + list(BUCKETIZED_TF_COLUMNS.values()) + list(CROSSED_TF_COLS)\n",
    "print(WIDE_TF_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=WIDE_TF_COLUMNS,\n",
    "    dnn_feature_columns=DEEP_TF_COLUMNS,\n",
    "    dnn_hidden_units=DNN_HIDDEN_UNITS\n",
    "    )\n",
    "  return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "#  train_file_name, test_file_name = maybe_download()\n",
    "  train_file_name=FLAGS.train_data;\n",
    "  test_file_name=FLAGS.test_data;\n",
    "  \n",
    "  print(\"train file = %s\" % train_file_name)\n",
    "  print(\"test file = %s\" % test_file_name)\n",
    "\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "    \n",
    "  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_outcome():\n",
    "  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "  print('model_dir = %s' % model_dir);\n",
    "  m = build_estimator(model_dir)\n",
    "\n",
    "  indata=StringIO(FLAGS.input_record)\n",
    "\n",
    "  prediction_set = pd.read_csv(\n",
    "      indata,\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=0,\n",
    "      engine=\"python\")\n",
    "\n",
    "  prediction_set[LABEL_COLUMN] = (\n",
    "      prediction_set[FLAGS.comparison_column].apply(lambda x: FLAGS.criteria in x)).astype(int)\n",
    "\n",
    "\n",
    "  y=m.predict(input_fn=lambda: input_fn(prediction_set))\n",
    "  print('Predictions: {}'.format(str(y)))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  if FLAGS.predict == \"true\":\n",
    "    predict_outcome()\n",
    "  else:\n",
    "    train_and_eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
